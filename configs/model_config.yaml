# Model config placeholder
# configs/model_config.yaml

architectures:
  - LlamaForCausalLM

model_type: llama
hidden_size: 2048
num_attention_heads: 16
num_hidden_layers: 24
intermediate_size: 5504
rms_norm_eps: 1e-6
vocab_size: 32000
tie_word_embeddings: true

initializer_range: 0.02
max_position_embeddings: 2048
use_cache: true
pad_token_id: 0
bos_token_id: 1
eos_token_id: 2

# QLoRA specific (optional tuning layer hint)
quantization_config:
  load_in_4bit: true
  bnb_4bit_compute_dtype: float16
  bnb_4bit_use_double_quant: true
  bnb_4bit_quant_type: nf4
